# Using Q-Learning to tune control parameters(Status): Still a work in progress.In this project, we implement Q-Learning to tune gain parameters in Proportional-Derivative (PD) Control of a particle in free space.For each learning trial, new control parameters are selected and a target (shown in red below) is randomly positioned in space. The learning agent (in blue) attempts the track the target throughout the trial. The error is accumulated and used to compute a reward signal. This reward signal is used to update the Q-table.The exploration rate is decreased exponentially with time, in order to exploit the learned parameters with greater frequency as the agent learns. ## Initial plotsHere is an animation of the particle learning:<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/animation_05.gif" width="80%" /></p>Note that the cost decreases with time:<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/cost_05.png" width="80%" /></p>and, as we reduce the exploration rate, the rewards grow with time:<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/rewards_05.png" width="80%" /></p>