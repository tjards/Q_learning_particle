# Embracing the Value in Hallucinations: Neural Dyna Q-Learning for Easy Control Problems**(Status)**: *Not ready yet. Q-Learning works well for tuning parameters and the neural network works well(ish) for modelling dynamics. Need to integrate into a need Dyna Q-Learning framework.*As descibed in Ref. 1, dyna-type reinforcement learning (RL) strategies greatly improve sample efficiency by learning from "simulated" experience. When an exact first-principles model is not available, these simulations are often implemented using data-driven models. In such cases, even small modelling errors can result in poor learning outcomes. These misleading simulated experiences can be considered "hallucinations" and, for certain applications, can be a big problem.In this project, we show that by appropriately structuring the learning process, we can avoid the pitfalls of such halluciations in automatic control problems. By constraining our RL problem within the well-established principles of proportion-integral-derivative (PID) control, we show that our proposed approach is sufficiently robust to absorb modelling errors in a Dyna Q-Learning framework. We implement a simple Neural Network to model a dynamic agent in (oh gosh) real-time and use this model as the basis for dyna planning. An off-policy Dyan-Q-Learning approach (similar to the on-policy Learning Automata approach at Ref. 2) is then used to tune the gain parameters for the agent controller.## References1. Taher Jafferjee, Ehsan Imani, Erin J. Talvitie, Martha White, and Michael Bowling. [Hallucinating Value: A Pitfall of Dyna-style Planning with Imperfect Environment Models](https://arxiv.org/pdf/2006.04363.pdf)2. Peter T. Jardine, Sidney N. Givigi, and Shahram Yousefi. [Leveraging Data Engineering to Improve Unmanned Aerial Vehicle Control Design](https://ieeexplore.ieee.org/document/9130726)## Initial results: Q-Learning Here is an animation of the agent learning:<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/animation_05.gif" width="80%" /></p>Note that the cost decreases with time. The noise is generated by the random motion of the target. In practice, what is important is that the variance of this noise decreases with time (because the agent is getting better at tracking the target as it moves).<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/cost_05.png" width="80%" /></p>As we reduce the exploration rate, the rewards grow with time (because the agent is exploiting the good parameters more often):<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/rewards_05.png" width="80%" /></p>## Initial results: Neural Network<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/batch1.png" width="30%" />  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/batch2.png" width="30%" />    <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/batch3.png" width="30%" /></p><p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/posError.png" width="80%" /></p>## Next steps