# Using Q-Learning to tune control parameters(Status): Still a work in progress.In this project, we implement Q-Learning to tune gain parameters in Proportional-Derivative (PD) Control of a particle in free space.For each learning trial: - Control parameters are randomly selected from a finite set of options.- A target (shown in red below) is randomly positioned in space. - The learning agent (in blue) attempts to the track the target throughout the trial. - The error is accumulated and used to compute a reward signal. - This reward signal is used to update the Q-table.- The exploration rate is decreased exponentially with time, in order to exploit the learned parameters with greater frequency as the agent learns. ## Initial plotsHere is an animation of the agent learning:<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/animation_05.gif" width="80%" /></p>Note that the cost decreases with time. The noise is generated by the random motion of the target. In practice, what is important is that the variance of this noise decreases with time (because the agents is getting better at tracking the target as it moves).<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/cost_05.png" width="80%" /></p>As we reduce the exploration rate, the rewards grow with time (because the agent is exploiting the good parameters more often):<p float="center">  <img src="https://github.com/tjards/Q_learning_particle/blob/master/Figs/rewards_05.png" width="80%" /></p>